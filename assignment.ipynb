{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to ðŸ¤— Transformers\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe23-TFb-NLP/blob/master/assignment.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "% pip install torch datasets tokenizers transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Premise\n",
    "\n",
    "This notebook will guide you through the process of finetuning a transformer model using the ðŸ¤— Transformers library.\n",
    "\n",
    "First, we need to select a task and suitable dataset. Here, we will use the [Textual Entailment or Natrual Language Inference](https://cims.nyu.edu/~sbowman/multinli/) task as an example. A suitable dataset can be found in the [GLUE repository on the ðŸ¤— Hub](https://huggingface.co/datasets/glue). The whole MNLI dataset ist way too big, so we will only use a slice of it.\n",
    "\n",
    "We can load the MNLI (slice) of the GLUE dataset using [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 39270\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnli_dataset = load_dataset(\"glue\", \"mnli\", split={\"train\": \"train[:10%]\", \"validation\": \"validation_matched\", \"test\": \"test_matched\"})\n",
    "print(mnli_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see above, the dataset is already split into train, development and test splits.\n",
    "Each row contains four, but we only need to focus the premise, hypothesis and the label.\n",
    "\n",
    "The textual entailment task requires us to recognize, given two text fragments, whether the meaning of one text is entailed (*can be inferred*) from the other text.\n",
    "\n",
    "In this example, we will use a BERT-family model. With BERT, we formulate the entailment task as a simple classification task by concatenating the premise and hypothesis and training our classifier on the first token (the `[CLS]` token) of the input string:\n",
    "\n",
    "```\n",
    "\"[CLS] This is the premise, i.e. a text that means something. [SEP] This is the hypothesis, i.e. what we may be able to infer [SEP]\"\n",
    "```\n",
    "\n",
    "But let's first take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.', 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him'], 'hypothesis': ['Product and geography are what make cream skimming work. ', 'You lose the things to the following level if the people recall.'], 'label': [1, 0], 'idx': [0, 1]}\n",
      "{'premise': ['The new rights are nice enough', 'This site includes a list of all award winners and a searchable database of Government Executive articles.'], 'hypothesis': ['Everyone really likes the newest benefits ', 'The Government Executive articles housed on the website are not able to be searched.'], 'label': [1, 2], 'idx': [0, 1]}\n",
      "{'premise': ['Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for.', 'The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds.'], 'hypothesis': ['Hierbas is a name worth looking out for.', 'Many people would be very unhappy to loose control over their own money.'], 'label': [-1, -1], 'idx': [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(mnli_dataset['train'][:2])\n",
    "print(mnli_dataset['validation'][:2])\n",
    "print(mnli_dataset['test'][:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we see above, the `test_matched` split contains **unlabeled** samples, be we can ignore that for now.\n",
    "\n",
    "Let's construct the sentences as we outlined above.\n",
    "\n",
    "*Note:* The `[CLS]` and final `[SEP]` will be added by the BERT's tokenizer, so we omit them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6622b1b9b71e4bd19dcd4cd2b95833c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 0], 'input': ['Conceptually cream skimming has two basic dimensions - product and geography. [SEP] Product and geography are what make cream skimming work. ', 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him [SEP] You lose the things to the following level if the people recall.']}\n"
     ]
    }
   ],
   "source": [
    "prepared_dataset = mnli_dataset.map(\n",
    "    lambda sample: {'input': f\"{sample['premise']} [SEP] {sample['hypothesis']}\"},\n",
    "    remove_columns=['premise', 'hypothesis', 'idx']\n",
    ")\n",
    "print(prepared_dataset['train'][:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Loading Pre-Trained Models\n",
    "\n",
    "Now we need to load a pre-trained BERT model. You should use a subclass of [AutoModel](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial).\n",
    "\n",
    "#### Load and instantiate a model for the textual entailment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer  #, AutoModelFor?\n",
    "\n",
    "config = ...  # TODO\n",
    "tokenizer = ...  # TODO\n",
    "model = ...  # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we could use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for easy training. You can follow the tutorial from [the official documentation](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop).\n",
    "\n",
    "#### Write the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(...)\n",
    "\n",
    "# TODO\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# TODO: evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Custom Training\n",
    "While using the trainer class is very convenient, if you have to run custom procedures during training, a regular training loop can be more accessible.\n",
    "\n",
    "We do need to do our own tokenization, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_pt(batch: dict):\n",
    "    return tokenizer(\n",
    "        batch['input'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "pt_dataset = prepared_dataset.map(encode_pt)\n",
    "print(pt_dataset['train'][:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "However, in a manual training loop, we will want to make use of PyTorch's DataLoaders, which require some extra care to collate batches with samples of different lengths.\n",
    "\n",
    "#### Implement `custom_collate`:\n",
    "- Pad and stack the `input_ids` in a tensor.\n",
    "- Stack the labels in a tensor of type `long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def custom_collate(batch: list[dict]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_ids = ...\n",
    "    label = ...\n",
    "    return input_ids, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Write the training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *  # TODO\n",
    "from torch.nn import *  # TODO\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "criterion = ...  # TODO\n",
    "optimizer = ...  # TODO\n",
    "num_epochs = ...  # TODO\n",
    "batch_size = ...  # TODO\n",
    "\n",
    "train_dataloader = DataLoader(pt_dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "dev_dataloader = DataLoader(pt_dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "model.to(device)\n",
    "for epoch in trange(num_epochs, position=0):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, position=1, leave=False):\n",
    "        ...  # TODO\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(dev_dataloader, position=1, leave=False):\n",
    "        ...  # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
